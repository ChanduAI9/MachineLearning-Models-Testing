{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "phDsud9mnj5Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_data = pd.read_excel('train.xlsx')\n",
        "print(\"Training Data:\")\n",
        "print(train_data.head())\n",
        "\n",
        "test_data = pd.read_excel('test.xlsx')\n",
        "print(\"Test Data:\")\n",
        "print(test_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j3LbEYZapEg",
        "outputId": "0ba34c05-28ae-49a8-d304-4c9e50faebb8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "   T1  T2  T3  T4  T5  T6  T7  T8  T9  T10  T11  T12  T13  T14  T15  T16  T17  \\\n",
            "0 -70 -61 -66 -53 -51 -63 -82 -57 -76  -78  -66  -66  -61  -59  -73  -75  -63   \n",
            "1 -77 -74 -71 -76 -65 -63 -66 -52 -55  -75  -72  -75  -74  -61  -64  -63  -53   \n",
            "2 -53 -38 -55 -66 -62 -62 -65 -70 -62  -52  -56  -53  -66  -68  -72  -60  -68   \n",
            "3 -72 -62 -59 -65 -65 -65 -78 -82 -83  -59  -84  -60  -64  -83  -69  -72  -95   \n",
            "4 -67 -69 -65 -63 -59 -53 -70 -72 -71  -60  -61  -57  -54  -76  -61  -66  -71   \n",
            "\n",
            "   T18 target  \n",
            "0  -77    B37  \n",
            "1  -63    B61  \n",
            "2  -77    A19  \n",
            "3  -73    A22  \n",
            "4  -80    A33  \n",
            "Test Data:\n",
            "   T1  T2  T3  T4  T5  T6  T7  T8  T9  T10  T11  T12  T13  T14  T15  T16  T17  \\\n",
            "0 -76 -83 -70 -66 -64 -72 -64 -69 -60  -76  -83  -78  -81  -81  -81  -70  -60   \n",
            "1 -58 -57 -78 -81 -73 -73 -78 -78 -82  -49  -55  -58  -66  -79  -72  -83  -74   \n",
            "2 -70 -70 -71 -69 -69 -68 -61 -55 -53  -82  -87  -76  -68  -57  -64  -75  -57   \n",
            "3 -71 -61 -56 -56 -61 -60 -68 -66 -72  -58  -55  -56  -58  -62  -61  -59  -64   \n",
            "4 -72 -71 -64 -69 -64 -63 -61 -42 -55  -61  -69  -67  -63  -63  -55  -49  -49   \n",
            "\n",
            "   T18  \n",
            "0  -60  \n",
            "1  -80  \n",
            "2  -70  \n",
            "3  -65  \n",
            "4  -57  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# features and target variable\n",
        "X_train = train_data.drop(columns=['target'])\n",
        "y_train = train_data['target']\n",
        "\n",
        "X_test = test_data[X_train.columns]\n"
      ],
      "metadata": {
        "id": "Q2MZqll8awUg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing using pipeline\n",
        "categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
        "numerical_cols = X_train.select_dtypes(include=['number']).columns\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "laLgFTAsazlI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# logistic regression classifier\n",
        "classifier = LogisticRegression(random_state=42)\n",
        "classifier.fit(X_train_preprocessed, y_train)\n",
        "\n",
        "y_train_pred = classifier.predict(X_train_preprocessed)\n",
        "\n",
        "#  classifier\n",
        "print(\"Training Data Classification Report:\")\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "print(\"Training Data Accuracy Score:\", accuracy_score(y_train, y_train_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7EoHuHha2qg",
        "outputId": "da74d97b-944a-4b46-b274-5f281683fd1b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          A1       0.93      0.91      0.92       215\n",
            "         A10       0.83      0.80      0.82       204\n",
            "         A11       0.95      1.00      0.97       212\n",
            "         A12       1.00      1.00      1.00       203\n",
            "         A13       1.00      1.00      1.00       219\n",
            "         A14       0.99      1.00      1.00       418\n",
            "         A15       0.99      0.95      0.97       413\n",
            "         A16       1.00      1.00      1.00       210\n",
            "         A17       0.97      0.93      0.95       204\n",
            "         A18       0.99      1.00      1.00       189\n",
            "         A19       0.99      1.00      0.99       208\n",
            "          A2       1.00      1.00      1.00       204\n",
            "         A20       0.99      1.00      1.00       205\n",
            "         A21       0.89      0.93      0.91       411\n",
            "         A22       1.00      1.00      1.00       210\n",
            "         A23       1.00      1.00      1.00       202\n",
            "         A24       1.00      1.00      1.00       211\n",
            "         A25       0.91      0.88      0.89       427\n",
            "         A26       1.00      1.00      1.00       203\n",
            "         A27       1.00      0.99      1.00       191\n",
            "         A28       0.99      1.00      0.99       206\n",
            "         A29       0.90      0.82      0.85       432\n",
            "          A3       0.90      0.92      0.91       429\n",
            "         A30       1.00      1.00      1.00       208\n",
            "         A31       0.85      0.99      0.92       212\n",
            "         A32       1.00      1.00      1.00       200\n",
            "         A33       0.89      0.92      0.90       633\n",
            "         A34       0.96      0.98      0.97       418\n",
            "         A35       0.97      0.98      0.98       199\n",
            "         A36       0.99      1.00      0.99       206\n",
            "         A37       0.95      0.95      0.95       415\n",
            "         A38       1.00      1.00      1.00       202\n",
            "         A39       0.93      0.86      0.89       635\n",
            "          A4       0.97      1.00      0.98       418\n",
            "         A40       0.98      0.99      0.98       199\n",
            "         A41       0.94      0.95      0.95       219\n",
            "         A42       0.99      1.00      0.99       218\n",
            "         A43       1.00      1.00      1.00       215\n",
            "         A44       1.00      1.00      1.00       213\n",
            "         A45       0.96      0.92      0.94       212\n",
            "         A46       1.00      1.00      1.00       203\n",
            "         A47       1.00      1.00      1.00       216\n",
            "         A48       0.96      0.99      0.98       218\n",
            "         A49       0.94      0.95      0.94       202\n",
            "          A5       0.85      0.86      0.86       210\n",
            "         A50       1.00      1.00      1.00       202\n",
            "         A51       1.00      1.00      1.00       220\n",
            "         A52       1.00      0.99      1.00       197\n",
            "         A53       0.98      0.97      0.97       217\n",
            "         A54       1.00      1.00      1.00       217\n",
            "         A55       1.00      1.00      1.00       206\n",
            "         A56       1.00      1.00      1.00       203\n",
            "         A57       1.00      0.99      1.00       212\n",
            "         A58       1.00      1.00      1.00       193\n",
            "         A59       1.00      1.00      1.00       232\n",
            "          A6       1.00      1.00      1.00       205\n",
            "         A60       1.00      1.00      1.00       197\n",
            "         A61       0.69      0.69      0.69       214\n",
            "         A62       1.00      1.00      1.00       209\n",
            "         A63       1.00      1.00      1.00       216\n",
            "         A64       1.00      1.00      1.00       214\n",
            "         A65       0.98      0.96      0.97       201\n",
            "         A66       1.00      1.00      1.00       204\n",
            "         A67       1.00      1.00      1.00       219\n",
            "         A68       1.00      1.00      1.00       206\n",
            "         A69       0.99      1.00      1.00       190\n",
            "          A7       0.98      1.00      0.99       216\n",
            "         A70       1.00      1.00      1.00       213\n",
            "         A71       1.00      1.00      1.00       197\n",
            "         A72       0.96      0.97      0.96       203\n",
            "         A73       1.00      0.99      0.99       217\n",
            "         A74       0.97      0.99      0.98       214\n",
            "         A75       1.00      1.00      1.00       224\n",
            "         A76       1.00      1.00      1.00       216\n",
            "         A77       0.97      0.98      0.98       241\n",
            "         A78       1.00      1.00      1.00       210\n",
            "         A79       0.99      1.00      1.00       205\n",
            "          A8       1.00      1.00      1.00       209\n",
            "         A80       0.99      1.00      1.00       205\n",
            "          A9       0.92      0.85      0.88       188\n",
            "          B1       0.99      1.00      1.00       208\n",
            "         B10       1.00      1.00      1.00       202\n",
            "         B11       1.00      1.00      1.00       210\n",
            "         B12       1.00      1.00      1.00       209\n",
            "         B13       0.93      0.97      0.95       230\n",
            "         B14       1.00      1.00      1.00       211\n",
            "         B15       1.00      1.00      1.00       200\n",
            "         B16       1.00      1.00      1.00       221\n",
            "         B17       0.65      0.60      0.62       209\n",
            "         B18       1.00      1.00      1.00       217\n",
            "         B19       1.00      1.00      1.00       206\n",
            "          B2       1.00      1.00      1.00       212\n",
            "         B20       1.00      1.00      1.00       212\n",
            "         B21       1.00      1.00      1.00       220\n",
            "         B22       1.00      1.00      1.00       223\n",
            "         B23       1.00      1.00      1.00       209\n",
            "         B24       1.00      1.00      1.00       211\n",
            "         B25       0.99      0.99      0.99       215\n",
            "         B26       1.00      1.00      1.00       208\n",
            "         B27       1.00      1.00      1.00       212\n",
            "         B28       1.00      1.00      1.00       212\n",
            "         B29       0.97      0.98      0.97       213\n",
            "          B3       1.00      1.00      1.00       215\n",
            "         B30       1.00      1.00      1.00       210\n",
            "         B31       1.00      1.00      1.00       216\n",
            "         B32       1.00      1.00      1.00       195\n",
            "         B33       1.00      1.00      1.00       213\n",
            "         B34       1.00      1.00      1.00       217\n",
            "         B35       1.00      1.00      1.00       212\n",
            "         B36       1.00      1.00      1.00       205\n",
            "         B37       1.00      1.00      1.00       209\n",
            "         B38       1.00      1.00      1.00       202\n",
            "         B39       1.00      1.00      1.00       208\n",
            "          B4       1.00      1.00      1.00       223\n",
            "         B40       1.00      1.00      1.00       205\n",
            "         B41       1.00      1.00      1.00       202\n",
            "         B42       1.00      1.00      1.00       217\n",
            "         B43       1.00      1.00      1.00       208\n",
            "         B44       1.00      1.00      1.00       218\n",
            "         B45       0.99      0.99      0.99       428\n",
            "         B46       1.00      1.00      1.00       216\n",
            "         B47       1.00      1.00      1.00       223\n",
            "         B48       1.00      1.00      1.00       229\n",
            "         B49       1.00      1.00      1.00       212\n",
            "          B5       1.00      1.00      1.00       215\n",
            "         B50       1.00      1.00      1.00       208\n",
            "         B51       1.00      1.00      1.00       230\n",
            "         B52       1.00      1.00      1.00       212\n",
            "         B53       0.95      0.94      0.95       218\n",
            "         B54       1.00      1.00      1.00       208\n",
            "         B55       1.00      1.00      1.00       201\n",
            "         B56       1.00      1.00      1.00       218\n",
            "         B57       0.98      0.95      0.96       214\n",
            "         B58       1.00      1.00      1.00       422\n",
            "         B59       1.00      1.00      1.00       200\n",
            "          B6       1.00      1.00      1.00       209\n",
            "         B60       1.00      1.00      1.00       212\n",
            "         B61       0.97      0.99      0.98       214\n",
            "         B62       1.00      1.00      1.00       220\n",
            "         B63       1.00      1.00      1.00       222\n",
            "         B64       1.00      1.00      1.00       206\n",
            "         B65       0.97      0.92      0.95       191\n",
            "         B66       1.00      1.00      1.00       193\n",
            "         B67       1.00      1.00      1.00       214\n",
            "         B68       1.00      1.00      1.00       197\n",
            "         B69       0.94      0.94      0.94       212\n",
            "          B7       1.00      1.00      1.00       222\n",
            "         B70       1.00      1.00      1.00       205\n",
            "         B71       1.00      1.00      1.00       214\n",
            "         B72       1.00      1.00      1.00       209\n",
            "         B73       0.97      0.95      0.96       215\n",
            "         B74       0.99      0.98      0.99       194\n",
            "         B75       1.00      1.00      1.00       200\n",
            "         B76       1.00      1.00      1.00       226\n",
            "         B77       0.96      0.99      0.97       192\n",
            "         B78       1.00      1.00      1.00       209\n",
            "         B79       0.99      1.00      0.99       216\n",
            "          B8       1.00      1.00      1.00       214\n",
            "         B80       1.00      1.00      1.00       207\n",
            "          B9       1.00      1.00      1.00       211\n",
            "\n",
            "    accuracy                           0.98     36752\n",
            "   macro avg       0.98      0.98      0.98     36752\n",
            "weighted avg       0.98      0.98      0.98     36752\n",
            "\n",
            "Training Data Accuracy Score: 0.9770896821941663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_test_pred = classifier.predict(X_test_preprocessed)\n",
        "\n",
        "test_data['Predicted Target'] = y_test_pred\n",
        "\n",
        "print(\"Test Data with Predictions:\")\n",
        "print(test_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9qmb4zLdHNF",
        "outputId": "15835bf0-cf96-4865-878c-a955dba9c958"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Data with Predictions:\n",
            "   T1  T2  T3  T4  T5  T6  T7  T8  T9  T10  T11  T12  T13  T14  T15  T16  T17  \\\n",
            "0 -76 -83 -70 -66 -64 -72 -64 -69 -60  -76  -83  -78  -81  -81  -81  -70  -60   \n",
            "1 -58 -57 -78 -81 -73 -73 -78 -78 -82  -49  -55  -58  -66  -79  -72  -83  -74   \n",
            "2 -70 -70 -71 -69 -69 -68 -61 -55 -53  -82  -87  -76  -68  -57  -64  -75  -57   \n",
            "3 -71 -61 -56 -56 -61 -60 -68 -66 -72  -58  -55  -56  -58  -62  -61  -59  -64   \n",
            "4 -72 -71 -64 -69 -64 -63 -61 -42 -55  -61  -69  -67  -63  -63  -55  -49  -49   \n",
            "\n",
            "   T18 Predicted Target  \n",
            "0  -60              B74  \n",
            "1  -80              A10  \n",
            "2  -70              B65  \n",
            "3  -65              B20  \n",
            "4  -57              A67  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zlYgMKfYymK3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}